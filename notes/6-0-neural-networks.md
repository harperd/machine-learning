# Neural Networks

*Neural Networks* (NNs) are much better for a complex non-linear hypothesis even when feature space is huge. They were originally motivated by looking at machines which replicate the brain's functionality. Neural networks were used a lot in the 80s and 90s with their popularity diminished in the late 90s. However, there has been a resurgence in neural networks because large scale NNs have became computationally feasible.

In neural networks, *layer 1* is the input layer followed by a number of *hidden layers* (layer 2 in the example below) and an *output layer* which produces the hypothesis result. Each layer is comprised of *activation functions* also called *neurons*. 

![Neural Network](../images/neural-network.png)

Neural networks can come in many different architectures such as below where there is an input layer, two hidden layers and an output layer:

![Neural Network](../images/neural-network-arch.png)


