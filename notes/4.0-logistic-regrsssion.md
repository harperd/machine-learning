# Logistic Regression

Two types of *Logistic Regression* or *classification*: *Binary* and *Multi Class*. Even though Logic Regression has the word "Regression" in the name, it is used to solve classification problems that are more complex and non-linear in nature. For example, the below data plot shows features that cannot be fit using a linear model.

![Logistic Regression](../images/logistic-regression.png)

## Binary Classification

Solves classification problems (two classes). For example, trying to determine if email is spam or not spam.

> $y\in\{0,1\}$

Given them email example $y=0$ could mean no spam where as $y=1$ could mean the email is spam.

How to determine if $y$ is $0$ or $1$? By using a *threshold* classifier such that our hypothesis function $h_\theta(x)$ predicts a value between $0$ and $1$. Such that:

> $0\le h_\theta(x)\le1$

For example:

> If $h_\theta(x)\ge  0.50$; Then predict $y=1$
>
> If $h_\theta(x)\lt  0.50$; Then predict $y=0$

To make predictions in this range we will need to use the *Sigmoid* function, also called the *Logistic* function.  Recall the hypothesis function for liner regression:

> $h_\theta(x)= \theta^tx$

The *Sigmoid* Function ($g$) takes our hypothesis ($z$) as a parameter and returns a value between $0$ and $1$. For example:

> $z = h_\theta(x) \therefore g(z) = \frac{1}{1+e^-z}$

Below shows a plot of the *Sigmoid* function that shows the *threshold* at $0.5$.

![Sigmoid Regression](../images/sigmoid-function.png)

In interpreting the value returned by the *Sigmoid* function, if it returns $y = .70$ when we say there is a $70\%$ chance that $y=1$ and therefore a $30\%$ chance that $y=0$. In a more concrete example, if malignant tumors lie in the range *above* $0.5$:

> $\overrightarrow x = \begin{bmatrix}{x_0\\x_1}\end{bmatrix}=\begin{bmatrix}{1\\tumorSize}\end{bmatrix}$

If $h_\theta(x) = 0.7$ then there is a $70\%$ change of the tumor might be malignant and since $.7 > .5$ we say that $y=1$. More formally, we write this in mathematical terms as:

> $h_\theta(x) = P(y=1|x;0)$

Where we say, the *probability* that $y=1$, given feature $x$, parameterized by $\theta$. Therefore:

> $P(y=0|x;\theta) + P(y=1|x;\theta)=1 \therefore .7 + .3 = 1$

Likewise we can find the probability of $y$ being zero by subtracting the $y=1$ probability from $1$:

> $(y=0|x;\theta)=1-P(y=1|x;\theta) \therefore$

***TODO:*** *Learning algorithm for fitting the learning parameters theta* $\theta$.