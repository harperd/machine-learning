{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "univariate-linear-regression-with-gradient-descent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harperd/machine-learning/blob/master/notebooks/univariate-linear-regression-with-gradient-descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY5PT6bd4V65",
        "colab_type": "text"
      },
      "source": [
        "# Univariate Linear Regression and Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY0aXI4RCl7E",
        "colab_type": "text"
      },
      "source": [
        "## Import Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kq34XhwCpyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NumPy adds support for large, multi-dimensional arrays and matrices, along with a large collection \n",
        "# of high-level mathematical functions to operate on these arrays.\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib is a plotting library for the Python programming language and its numerical mathematics \n",
        "# extension NumPy. It provides an object-oriented API for embedding plots into applications using \n",
        "# general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pandas is a software library for data manipulation and analysis. In particular, it offers data \n",
        "# structures and operations for manipulating numerical tables and time series.\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zek8oIAP5YQH",
        "colab_type": "text"
      },
      "source": [
        "# Hypothesis Function\n",
        "\n",
        "In machine learning, a *hypothesis* function is used to predict outcomes or $y$ values. Below is the univariate linear regression hypothesis function where theta ($\\theta$) can represent any two numbers and $h_\\theta(x)$ or $y$ is our *prediction*. We just have to figure out what those two numbers are that allow the function to best intersect our data or features. It's a simple linear equation but finding the best theta values is where the challenge lies.\n",
        "\n",
        "> $h_{\\theta }( x) =\\theta _{0} \\ +\\ \\theta _{1} x$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFuhgcny3XzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hypothesis Function - This function will compute predictions for all features. \n",
        "#\n",
        "# Parameters\n",
        "#     theta: The two theta values as an array [ theta1, theta2 ]\n",
        "#     X: A DataFrame containing the features to create predictions against.\n",
        "def hypothesis(theta, X):\n",
        "    # Here, X is a Pandas DataFrame containing all of our data.\n",
        "    # X.shape returns tuple of the shape's dimentions in rows x columns.\n",
        "    # Therefore, X.shape[0] will get us the number of rows, or features.\n",
        "    num_features = X.shape[0]\n",
        "    \n",
        "    # numpy.ones returns a new array of given shape, filled with ones.\n",
        "    # Here, we want a new array (vector) of size num_features x 1 (rows x columns).\n",
        "    h = np.ones(num_features, 1)\n",
        "    \n",
        "    # Loop through each feature (x) in our data set.\n",
        "    #\n",
        "    # TODO: Perform this in a more optimal manner using matrix multiplication\n",
        "    #       against the entire feature set, X.\n",
        "    for i in range(0, num_features):\n",
        "        # Get the next feature value.\n",
        "        feature_value = X[i]\n",
        "        \n",
        "        # Here, we are going to concatinate a 1x1 vector containing the value \n",
        "        # of 1 with a 1x1 vector containing the feature value with the result \n",
        "        # being a 1x2 vector.\n",
        "        #\n",
        "        # After the concatination, our 1x2 vector resembles the below, if our\n",
        "        # feature_value = 3:\n",
        "        #\n",
        "        #    x = [ 1 3 ]\n",
        "        #\n",
        "        vector_one = np.ones(1)\n",
        "        vector_feature = np.array(feature_value)\n",
        "        \n",
        "        x = np.concatenate((vector_one, vector_feature), axis = 0)\n",
        "        \n",
        "        # Finally, we multiply the vector x with our two theta values and\n",
        "        # assign it to h[i]:\n",
        "        #\n",
        "        # h[i] = [ theta1 theta2 ] * [ 1 feature ]\n",
        "        #\n",
        "        h[i] = float(np.matmul(theta, x))\n",
        "    \n",
        "    # Here, we have a vector (h) with our predictions with a size of \n",
        "    # num_features x 1 (rows x columns). we need to reshape the vector so it \n",
        "    # matches the same number of rows as our DataFrame, X.\n",
        "    #\n",
        "    # TODO: Why do we need to reshape?\n",
        "    h = h.reshape(num_features)\n",
        "    \n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FlSz41X0sSx",
        "colab_type": "text"
      },
      "source": [
        "We can choose good theta values by using the *Cost Function* denoted as $J(\\theta_{0}, \\theta_{1})$ where  $\\theta_{0}$, and $\\theta_{1}$ points on the $x$,$y$ axis and $J(\\theta_{0}, \\theta_{1})$ is *z*. This is also called the *Squared Error Function* which is the most commonly used for linear regression problems. Here, we want to get the results of our cost function as close to zero as possible by trying different values for $\\theta _{0}$ and $\\theta _{1}$.\n",
        "\n",
        "> $\\large J( \\theta _{0} ,\\ \\theta _{1}) =\\frac{1}{2m}\\sum\\limits ^{m}_{i=1}\\left( h_{\\theta }\\left( x^{( i)}\\right) -y^{( i)}\\right)^{2}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOZtjTK-0tj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_cost(theta, X, y):\n",
        "  m = len(y)\n",
        "  # Perform matrix multiplication between our feature set X and theta values.\n",
        "  predictions = hypothesis(theta, X)\n",
        "  # Compute the cost of the predictions\n",
        "  cost = (1/2*m) + np.sum(np.square(predictions - y))\n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNVDoV-Kypjm",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "What *Gradient Descent* algorithm does is *simultaneously* compute values for $\\theta_{0}$ and $\\theta_{1}$. What is meant by *simultaneously* is represented in the pseudo code below where $\\theta_{0}$ and $\\theta_{1}$ are assigned new values at the same time. In other words, if $\\theta_{0}$ was set ($\\theta_{0} :=$ *temp0*) *before* temp1 was set (*temp1* $:= \\theta_{1}-\\alpha\\frac{\\partial}{\\partial\\theta_{1}}J(\\theta_{0},\\theta_{1})$) then it would affect the results of temp1 and yield incorrect results. We want to repeat this series of steps until we reach *convergence* or $\\theta_{0}$ and $\\theta_{1}$ are at their minimum.\n",
        "\n",
        ">*repeat until convergence {* \n",
        "\n",
        ">$temp 0:= \\theta_{0}-\\alpha\\frac{1}{m}\\sum\\limits ^{m}_{i=1}\\left( h_{\\theta }\\left( x^{(i)}\\right) -y^{( i)}\\right)$\n",
        "\n",
        ">$temp1 := \\theta_{1}-\\alpha\\frac{1}{m}\\sum\\limits ^{m}_{i=1}\\left( h_{\\theta }\\left( x^{(i)}\\right) -y^{( i)}\\right)\\cdot x^{(i)}$\n",
        "\n",
        ">$\\theta_{0} := temp0$\n",
        "\n",
        ">$\\theta_{1} := temp1$\n",
        "\n",
        ">*}* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-PzzJB86cP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stochastic Gradient Descent Algorithm - Performs Gradient Descent.\n",
        "#\n",
        "# Parameters\n",
        "#     theta: The two theta values as an array [ theta1, theta2 ]\n",
        "#     alpha: The learning rate.\n",
        "#     num_iters: The number of iterations to perform.\n",
        "#     p: The hypothesis predition.\n",
        "#     X: The feature set.\n",
        "#     y: The target variable set.\n",
        "def sgd(theta, alpha, num_iters, p, X, y):\n",
        "    m = len(y)\n",
        "    cost_history = np.zeros(iterations)\n",
        "    theta_history = np.zeros((iterations,2))\n",
        "    \n",
        "    for it in range(iterations):\n",
        "        prediction = np.dot(X, theta)\n",
        "        \n",
        "        theta = theta - (1 / m) * learning_rate * ( X.T.dot((prediction - y)) )\n",
        "        theta_history[it,:] =theta.T\n",
        "        cost_history[it]  = cal_cost(theta,X,y)\n",
        "        \n",
        "    return theta, cost_history, theta_history\n",
        "  \n",
        "    for i in range(0, num_iters):\n",
        "        theta[0] = theta[0] - (alpha) * (p - y)\n",
        "        theta[1] = theta[1] - (alpha) * ((p - y) * X)\n",
        "        p = theta[1] * X + theta[0] \n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDI2_-kJ6eh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd_linear_regression(X, y, alpha, num_iters):\n",
        "    # initializing the parameter vector...\n",
        "    theta = np.zeros(2)\n",
        "    # hypothesis calculation....\n",
        "    h = hypothesis(theta, X)    \n",
        "    # returning the optimized parameters by Gradient Descent...\n",
        "    for i in range(0, X.shape[0]):\n",
        "        theta = SGD(theta,alpha,num_iters,h[i],X[i],y[i])\n",
        "    theta = theta.reshape(1, 2)\n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tbEg1AB6hA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.loadtxt('data1.txt', delimiter=',')\n",
        "X_train = data[:,0] #the feature_set\n",
        "y_train = data[:,1] #the labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CukpwQ9N6jQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(X_train, y_train)\n",
        "plt.xlabel('Population of City in 10,000s')\n",
        "plt.ylabel('Profit in $10,000s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kg50QML6lWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calling the principal function with learning_rate = 0.0001 and \n",
        "# num_iters = 100000\n",
        "theta = sgd_linear_regression(X_train, y_train, 0.0001, 100000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}