{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "univariate-linear-regression-with-gradient-descent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harperd/machine-learning/blob/master/notebooks/univariate-linear-regression-with-gradient-descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY5PT6bd4V65",
        "colab_type": "text"
      },
      "source": [
        "# Univariate Linear Regression and Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY0aXI4RCl7E",
        "colab_type": "text"
      },
      "source": [
        "## Import Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kq34XhwCpyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NumPy adds support for large, multi-dimensional arrays and matrices, along with a large collection \n",
        "# of high-level mathematical functions to operate on these arrays.\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib is a plotting library for the Python programming language and its numerical mathematics \n",
        "# extension NumPy. It provides an object-oriented API for embedding plots into applications using \n",
        "# general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+.\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Pandas is a software library for data manipulation and analysis. In particular, it offers data \n",
        "# structures and operations for manipulating numerical tables and time series.\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zek8oIAP5YQH",
        "colab_type": "text"
      },
      "source": [
        "# Hypothesis Function\n",
        "\n",
        "In machine learning, a *hypothesis* function is used to predict outcomes or $y$ values. Below is the univariate linear regression hypothesis function where theta ($\\theta$) can represent any two numbers and $h_\\theta(x)$ or $y$ is our *prediction*. We just have to figure out what those two numbers are that allow the function to best intersect our data or features. It's a simple linear equation but finding the best theta values is where the challenge lies.\n",
        "\n",
        "> $h_{\\theta }( x) =\\theta _{0} \\ +\\ \\theta _{1} x$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFuhgcny3XzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hypothesis Function - This function will compute predictions for all features. \n",
        "#\n",
        "# Parameters\n",
        "#     theta: The two theta values as an array [ theta1, theta2 ]\n",
        "#     X: A DataFrame containing the features to create predictions against.\n",
        "def hypothesis(theta, X):\n",
        "    # Here, X is a Pandas DataFrame containing all of our data.\n",
        "    # X.shape returns tuple of the shape's dimentions in rows x columns.\n",
        "    # Therefore, X.shape[0] will get us the number of rows, or features.\n",
        "    num_features = len(X)\n",
        "    \n",
        "    # numpy.ones returns a new array of given shape, filled with ones.\n",
        "    # Here, we want a new array (vector) of size num_features x 1 (rows x columns).\n",
        "    h = np.ones((num_features, 1))\n",
        "    \n",
        "    # Loop through each feature (x) in our data set.\n",
        "    #\n",
        "    # TODO: Perform this in a more optimal manner using matrix multiplication\n",
        "    #       against the entire feature set, X.\n",
        "    for i in range(0, num_features):\n",
        "        # Get the next feature value.\n",
        "        feature_value = X[i]\n",
        "        \n",
        "        # Here, we are going to concatinate a 1x1 vector containing the value \n",
        "        # of 1 with a 1x1 vector containing the feature value with the result \n",
        "        # being a 1x2 vector.\n",
        "        #\n",
        "        # After the concatination, our 1x2 vector resembles the below, if our\n",
        "        # feature_value = 3:\n",
        "        #\n",
        "        #    x = [ 1 3 ]\n",
        "        #\n",
        "        vector_one = np.ones(1)\n",
        "        vector_feature = np.array(feature_value)\n",
        "        \n",
        "        x = np.concatenate((vector_one, vector_feature), axis = 0)\n",
        "        \n",
        "        # Finally, we multiply the vector x with our two theta values and\n",
        "        # assign it to h[i]:\n",
        "        #\n",
        "        # h[i] = [ theta1 theta2 ] * [ 1 feature ]\n",
        "        #\n",
        "        h[i] = float(np.matmul(theta, x))\n",
        "    \n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FlSz41X0sSx",
        "colab_type": "text"
      },
      "source": [
        "We can choose good theta values by using the *Cost Function* denoted as $J(\\theta_{0}, \\theta_{1})$ where  $\\theta_{0}$, and $\\theta_{1}$ points on the $x$,$y$ axis and $J(\\theta_{0}, \\theta_{1})$ is *z*. This is also called the *Squared Error Function* which is the most commonly used for linear regression problems. Here, we want to get the results of our cost function as close to zero as possible by trying different values for $\\theta _{0}$ and $\\theta _{1}$.\n",
        "\n",
        "> $\\large J( \\theta _{0} ,\\ \\theta _{1}) =\\frac{1}{2m}\\sum\\limits ^{m}_{i=1}\\left( h_{\\theta }\\left( x^{( i)}\\right) -y^{( i)}\\right)^{2}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOZtjTK-0tj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(theta, X, y):\n",
        "  # Get the number of examples\n",
        "  m = len(y)\n",
        "  \n",
        "  # Perform matrix multiplication between our feature set X and theta values.\n",
        "  predictions = hypothesis(theta, X)\n",
        "  \n",
        "  # Compute the cost of the predictions\n",
        "  cost = np.sum(np.square(predictions - y)) / (2*m)\n",
        "  \n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNVDoV-Kypjm",
        "colab_type": "text"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "What *Gradient Descent* algorithm does is *simultaneously* compute values for $\\theta_{0}$ and $\\theta_{1}$. What is meant by *simultaneously* is represented in the pseudo code below where $\\theta_{0}$ and $\\theta_{1}$ are assigned new values at the same time. In other words, if $\\theta_{0}$ was set ($\\theta_{0} :=$ *temp0*) *before* temp1 was set (*temp1* $:= \\theta_{1}-\\alpha\\frac{\\partial}{\\partial\\theta_{1}}J(\\theta_{0},\\theta_{1})$) then it would affect the results of temp1 and yield incorrect results. We want to repeat this series of steps until we reach *convergence* or $\\theta_{0}$ and $\\theta_{1}$ are at their minimum.\n",
        "\n",
        ">*repeat until convergence {* \n",
        "\n",
        ">$temp 0:= \\theta_{0}-\\alpha\\frac{1}{m}\\sum\\limits ^{m}_{i=1}\\left( h_{\\theta }\\left( x^{(i)}\\right) -y^{( i)}\\right)$\n",
        "\n",
        ">$temp1 := \\theta_{1}-\\alpha\\frac{1}{m}\\sum\\limits ^{m}_{i=1}\\left( h_{\\theta }\\left( x^{(i)}\\right) -y^{( i)}\\right)\\cdot x^{(i)}$\n",
        "\n",
        ">$\\theta_{0} := temp0$\n",
        "\n",
        ">$\\theta_{1} := temp1$\n",
        "\n",
        ">*}* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXSHg54IOXLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent2(theta, alpha, X, y):\n",
        "   # Get the number of examples\n",
        "    m = len(y)\n",
        "\n",
        "    converged = False\n",
        "    report = []\n",
        "    iteration = 1\n",
        "    p_theta = [ -1, -1 ]\n",
        "    temp = [ 0, 0 ]\n",
        "    \n",
        "    X[:,:-1] = np.ones(1)\n",
        "\n",
        "    # Keep trying until we have converged at the local minimum.\n",
        "    while(not converged):\n",
        "        # Try a prediction with current theta values.\n",
        "        # y = h(x)\n",
        "        predictions = hypothesis(theta, X)\n",
        "        \n",
        "        # Compute the cost i.e. J(theta0, theta1)\n",
        "        cost = compute_cost(theta, X, y)\n",
        "        \n",
        "        #temp[0]=theta[0]-(alpha)*((np.sum(predictions-y))/m)\n",
        "        #temp[1]=theta[1]-(alpha)*((np.sum((predictions-y)*X))/m)\n",
        "        #theta=temp\n",
        " \n",
        "\n",
        "        #temp[0] = theta[0] - alpha * (np.sum(predictions - y) / m)\n",
        "        #temp[1] = theta[1] - alpha * (np.sum(np.matmul(predictions - y, X.T)) / m)\n",
        "    \n",
        "        temp = theta - alpha * ((np.matmul(hypothesis(theta, predictions - y), X.T)) / m)\n",
        " \n",
        "        theta = temp\n",
        "        \n",
        "       # print('theta = ' + str(theta))\n",
        "       # print('p_theta = ' + str(p_theta))\n",
        "        \n",
        "        # Check to see if we have converged or not.\n",
        "        converged = theta[0] == p_theta[0] and theta[1] == p_theta[1]\n",
        "        \n",
        "        # Set the optimal theta to the current theta.\n",
        "        p_theta = theta\n",
        "        \n",
        "        # Get data to report\n",
        "        if(iteration % 20 == 0 or converged):\n",
        "          report.append([ \n",
        "              'Converged -->' if converged else '', \n",
        "              iteration, \n",
        "              cost, \n",
        "              theta[0], \n",
        "              theta[1]])\n",
        "\n",
        "        iteration = iteration + 1\n",
        "        \n",
        "    # Convert our report data to a Pandas DataFrame for better rendering.\n",
        "    df_report = pd.DataFrame(\n",
        "        report, \n",
        "        columns = [ 'Converged', 'Iteration', 'Cost', 'Theta0', 'Theta1' ]);\n",
        "\n",
        "    return theta, df_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-PzzJB86cP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function to see if we have converged to the local minimum.\n",
        "def is_converged(c_theta, p_theta):\n",
        "  return c_theta[0] == p_theta[0] and c_theta[1] == p_theta[1]\n",
        "\n",
        "# Gradient Descent Algorithm - Performs Gradient Descent.\n",
        "#\n",
        "# Parameters\n",
        "#     theta: The two theta values as an array [ theta1, theta2 ]\n",
        "#     alpha: The learning rate.\n",
        "#     iterations: The number of iterations to perform.\n",
        "#     X: The feature set.\n",
        "#     y: The target variable set.\n",
        "def gradient_descent(theta, alpha, X, y):\n",
        "    # Get the number of examples\n",
        "    m = len(y)\n",
        "\n",
        "    converged = False\n",
        "    report = []\n",
        "    optimal_theta = theta\n",
        "    iteration = 1\n",
        "\n",
        "    # Keep trying until we have converged at the local minimum.\n",
        "    while(not converged):\n",
        "        # Try a prediction with current theta values.\n",
        "        # y = h(x)\n",
        "        predictions = hypothesis(theta, X)\n",
        "        \n",
        "        # Compute the cost i.e. J(theta0, theta1)\n",
        "        cost = compute_cost(theta, X, y)\n",
        "        \n",
        "        # Compute the average gradient per example.\n",
        "        # 1/m * sum(h(x) - y) * X\n",
        "        gradient = np.dot(X.T, predictions - y) / m\n",
        "        \n",
        "        # Update theta\n",
        "        theta = ( theta - alpha * gradient )\n",
        "        theta = theta[len(theta) - 1]\n",
        "        \n",
        "        # Check to see if we have converged or not.\n",
        "        converged = is_converged(theta, optimal_theta)\n",
        "        \n",
        "        # Set the optimal theta to the current theta.\n",
        "        optimal_theta = theta\n",
        "        \n",
        "        # Get data to report\n",
        "        if(iteration % 20 == 0 or converged):\n",
        "          report.append([ \n",
        "              'Converged -->' if converged else '', \n",
        "              iteration, \n",
        "              cost, \n",
        "              optimal_theta[0], \n",
        "              optimal_theta[1]])\n",
        "\n",
        "        iteration = iteration + 1\n",
        "        \n",
        "    # Convert our report data to a Pandas DataFrame for better rendering.\n",
        "    df_report = pd.DataFrame(\n",
        "        report, \n",
        "        columns = [ 'Converged', 'Iteration', 'Cost', 'Theta0', 'Theta1' ]);\n",
        "\n",
        "    return optimal_theta, df_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_J5gBlV6aA1",
        "colab_type": "code",
        "outputId": "4ad3f035-e3cf-43c7-cfa5-79df2bfc4394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "# Generate random samples\n",
        "num_samples = 10\n",
        "\n",
        "# Generate some random inpendent variables (features)\n",
        "X = pd.DataFrame(2 * np.random.rand(num_samples, 1), columns = [ 'Feature' ])\n",
        "\n",
        "# Add a bias column of all ones to the beginning of our DataFrame\n",
        "# since theta0 will not have a feature.\n",
        "X.insert(0, 'bias', np.ones(num_samples))\n",
        "\n",
        "# Generate some random depenant variables\n",
        "X = X.join(pd.DataFrame(4 + 3 * X.as_matrix(columns=X.columns[1:]) + np.random.randn(num_samples, 1), columns = [ 'Outcome' ]))\n",
        "\n",
        "X\n",
        "                                                                \n",
        "# Plot our random samples\n",
        "#plt.plot(X.values, y.values, 'b.')\n",
        "#plt.xlabel(\"$x$\", fontsize=18)\n",
        "#plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "#_ =plt.axis([0,2,0,15])"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bias</th>\n",
              "      <th>Feature</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.075207</td>\n",
              "      <td>7.543828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.198582</td>\n",
              "      <td>6.538832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.669181</td>\n",
              "      <td>6.886811</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.731653</td>\n",
              "      <td>5.322457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.872674</td>\n",
              "      <td>7.166983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.110586</td>\n",
              "      <td>9.195941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.909329</td>\n",
              "      <td>9.539656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.764470</td>\n",
              "      <td>6.987822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.508291</td>\n",
              "      <td>7.277869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.822638</td>\n",
              "      <td>10.024566</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   bias   Feature    Outcome\n",
              "0   1.0  1.075207   7.543828\n",
              "1   1.0  1.198582   6.538832\n",
              "2   1.0  0.669181   6.886811\n",
              "3   1.0  0.731653   5.322457\n",
              "4   1.0  1.872674   7.166983\n",
              "5   1.0  1.110586   9.195941\n",
              "6   1.0  1.909329   9.539656\n",
              "7   1.0  0.764470   6.987822\n",
              "8   1.0  1.508291   7.277869\n",
              "9   1.0  1.822638  10.024566"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tSfsgQE-1rv",
        "colab_type": "code",
        "outputId": "a4f72c3e-575e-454a-d8e9-b4d803996a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "# Initialize theta and set our learning rate.\n",
        "theta = [ 0, 0 ]\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Perform Gradient Descent\n",
        "theta, report = gradient_descent(theta, learning_rate, X, y)\n",
        "\n",
        "# Show the Gradent Descent report\n",
        "pd.option_context('display.float_format', '{:,.20f}'.format)\n",
        "report"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-97-45caece3cf03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Perform Gradient Descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Show the Gradent Descent report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-4e0e76b84961>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(theta, alpha, X, y)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Try a prediction with current theta values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# y = h(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Compute the cost i.e. J(theta0, theta1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-882989f9300a>\u001b[0m in \u001b[0;36mhypothesis\u001b[0;34m(theta, X)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mvector_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Finally, we multiply the vector x with our two theta values and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP3l3FXScCQs",
        "colab_type": "code",
        "outputId": "b4b1df92-2dd2-4004-df8b-fe8ae7b3831e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "# Execute our hypothesis with our optimal theta values.\n",
        "training_predictions = hypothesis(theta, X)\n",
        "\n",
        "# Plot the results.\n",
        "plt.plot(X,y,'b.')\n",
        "plt.xlabel(\"$x$\", fontsize=18)\n",
        "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
        "plt.plot(X, training_predictions)\n",
        "_ =plt.axis([0,2,0,15])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAESCAYAAAD5d3KwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}