{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiclass-logistic-regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harperd/machine-learning/blob/master/notebooks/multiclass-logistic-regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMu0-xrhFSTR",
        "colab_type": "text"
      },
      "source": [
        "# Multiclass Logistic Regression\n",
        "\n",
        "Use logistic regression to recognize hand-written digits (0 to 9)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2GQSOFcC6Od",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIbeQTjIFLV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import pandas as pd\n",
        "import scipy.optimize as opt\n",
        "import google.colab as colab\n",
        "import io\n",
        "\n",
        "from scipy.io import loadmat\n",
        "\n",
        "# Allow saving our graphs in the notebook\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQEbEljkroV1",
        "colab_type": "text"
      },
      "source": [
        "Chart Dark Theme For Google Colab Dark Mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_h2iY6srm7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style.use('dark_background')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3ZqURKnCgsa",
        "colab_type": "text"
      },
      "source": [
        "## Read Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yto5KquyIjpU",
        "colab_type": "code",
        "outputId": "0ed2e863-eea2-40af-ed55-af695f403fd4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "mat_file = colab.files.upload()\n",
        "!ls -l"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-336557a9-9190-4a57-9908-833c6453266e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-336557a9-9190-4a57-9908-833c6453266e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ex3data1.mat to ex3data1.mat\n",
            "total 7340\n",
            "-rw-r--r-- 1 root root 7511764 Aug  4 14:31 ex3data1.mat\n",
            "drwxr-xr-x 1 root root    4096 Aug  1 16:08 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh72WN5fGNN8",
        "colab_type": "code",
        "outputId": "9d51b8be-b4d2-4231-f356-1bf6858c31e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "mat_data = loadmat('ex3data1.mat')\n",
        "mat_data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
              " '__globals__': [],\n",
              " '__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
              " '__version__': '1.0',\n",
              " 'y': array([[10],\n",
              "        [10],\n",
              "        [10],\n",
              "        ...,\n",
              "        [ 9],\n",
              "        [ 9],\n",
              "        [ 9]], dtype=uint8)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r2DqgKcJRXI",
        "colab_type": "code",
        "outputId": "4450c8a6-024d-4ca2-ce41-73418c6bb013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "X = mat_data['X']\n",
        "# Add bias column\n",
        "X = np.hstack((np.ones(X.shape[0])[:, np.newaxis], X))\n",
        "\n",
        "y = mat_data['y']\n",
        "theta = np.array(np.zeros(X.shape[1]), ndmin = 2)\n",
        "\n",
        "print(f'X Shape: {X.shape}')\n",
        "print(f'y Shape: {y.shape}')\n",
        "print(f'Theta Shape: {theta.shape}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X Shape: (5000, 401)\n",
            "y Shape: (5000, 1)\n",
            "Theta Shape: (1, 401)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3JyiN6oO92O",
        "colab_type": "text"
      },
      "source": [
        "![Hand written numbers](https://github.com/harperd/machine-learning/blob/master/images/ex3-1.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DagnmOHRJWaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  return 1 / ( 1 + np.exp(-z) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV1LSYsXPX8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_hypothesis(theta, X):\n",
        "  # Compute our hypothesis\n",
        "  z = X @ theta.T\n",
        "  \n",
        "  # Scale our hypothesis using Sigmoid\n",
        "  # Here, if the parameter is zero then the sigmoid value will be 0.5.\n",
        "  h = sigmoid(z)\n",
        "  \n",
        "  return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-HfxOfAPcW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(theta, X, y):\n",
        "  # Fix for minimize function\n",
        "  theta = np.array(theta, ndmin = 2)\n",
        "  X = np.array(X, ndmin = 2)\n",
        "  y = np.array(y, ndmin = 2)\n",
        "  \n",
        "  # Compute our hypothesis\n",
        "  h = compute_hypothesis(theta, X)\n",
        "  \n",
        "  first = np.log(h) * -y\n",
        "  second = np.log(1 - h) * (1 - y)\n",
        "  \n",
        "  # The number of examples\n",
        "  m = len(y)\n",
        "  \n",
        "  cost = np.sum(first - second) / m\n",
        "  \n",
        "  return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMmza1f3QjVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_gradient(theta, X, y):\n",
        "  # Fix for minimize function\n",
        "  theta = np.array(theta, ndmin = 2)\n",
        "  X = np.array(X, ndmin = 2)\n",
        "  y = np.array(y, ndmin = 2)\n",
        "  \n",
        "  # Compute our hypothesis\n",
        "  h = compute_hypothesis(theta, X)\n",
        "\n",
        "  # Get the error\n",
        "  error = h - y\n",
        "\n",
        "  # The number of examples\n",
        "  m = len(y)\n",
        "  \n",
        "  # Calculate the gradient\n",
        "  gradient = (( error.T @ X ) / m) + ((.1 / m) * theta)\n",
        "  \n",
        "  return gradient"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GVnvahignF7",
        "colab_type": "code",
        "outputId": "927cd96c-c1d7-4b00-e08a-6aa5cdef387e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        }
      },
      "source": [
        "def train_model(theta, X, y):\n",
        "  theta_min = {}\n",
        "  \n",
        "  for k in range(1, 11):\n",
        "    print(f'> Optimizing theta values for class {k}...')\n",
        "\n",
        "    y_train = [ 1 if K[0] == k else 0 for K in y ]\n",
        "    y_train = np.array(y_train, ndmin = 2).T\n",
        "\n",
        "    result = opt.fmin_tnc(\n",
        "      # Initial guess.\n",
        "      x0 = theta,\n",
        "      # Objective function to be minimized.\n",
        "      func = compute_cost,\n",
        "      # Gradient of func.\n",
        "      fprime = compute_gradient,\n",
        "      # Extra arguments passed to f and fprime.\n",
        "      args = ( X, y_train ))\n",
        "    \n",
        "    theta_min[k - 1] = result[0]\n",
        "\n",
        "    iterations = result[1]\n",
        "    cost = compute_cost(result[0], X, y_train)\n",
        "    \n",
        "    print(f'  Iterations = {iterations}, cost = {cost}')\n",
        "\n",
        "  return theta_min\n",
        "\n",
        "theta_min = train_model(theta, X, y)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> Optimizing theta values for class 1...\n",
            "  Iterations = 123, cost = 0.008345230609298671\n",
            "> Optimizing theta values for class 2...\n",
            "  Iterations = 215, cost = 0.04443323146861598\n",
            "> Optimizing theta values for class 3...\n",
            "  Iterations = 187, cost = 0.052502878719436374\n",
            "> Optimizing theta values for class 4...\n",
            "  Iterations = 171, cost = 0.026052862922637393\n",
            "> Optimizing theta values for class 5...\n",
            "  Iterations = 233, cost = 0.04710706870711914\n",
            "> Optimizing theta values for class 6...\n",
            "  Iterations = 178, cost = 0.011827903837127233\n",
            "> Optimizing theta values for class 7...\n",
            "  Iterations = 192, cost = 0.024533886541116552\n",
            "> Optimizing theta values for class 8...\n",
            "  Iterations = 182, cost = 0.0740679645728465\n",
            "> Optimizing theta values for class 9...\n",
            "  Iterations = 181, cost = 0.065474591260563\n",
            "> Optimizing theta values for class 10...\n",
            "  Iterations = 155, cost = 0.005009637890079448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Op70V9ty52Al",
        "colab_type": "code",
        "outputId": "5e831346-7078-4a6a-cf3a-ca58de5ec5ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def make_predictions(theta, X):\n",
        "  # Compute our hypothesis\n",
        "  h = compute_hypothesis(theta, X)\n",
        "  \n",
        "  # Make prediction of 1 or zero based on threshold.\n",
        "  predictions = [ 1 if s >= 0.5 else 0 for s in h ]\n",
        "\n",
        "  return predictions\n",
        "\n",
        "def compute_accuracy(predictions, y):\n",
        "  # Get the correct predictions where correct is 1 and\n",
        "  # incorrect is 0.\n",
        "  correct = [ \n",
        "      1 if p_val - y_val == 0 else 0 \n",
        "      # The purpose of zip() is to map the similar index of multiple \n",
        "      # containers so that they can be used just using as single entity.\n",
        "      for (p_val, y_val) in zip(predictions, y)\n",
        "  ]\n",
        "  \n",
        "  # Calculate the overall accuracy.\n",
        "  accuracy = sum(correct) / len(correct)\n",
        "  \n",
        "  return accuracy\n",
        "\n",
        "def compute_model_accuracy(theta_min, X, y):\n",
        "  accuracy = 0\n",
        "\n",
        "  for k in theta_min.keys():\n",
        "    theta = theta_min[k]\n",
        "    correct = [ 1 if K[0] == k else 0 for K in y ]\n",
        "    predictions = make_predictions(theta, X)\n",
        "    accuracy = accuracy + compute_accuracy(predictions, correct)\n",
        "\n",
        "  return ( accuracy / len(theta_min.keys()) ) * 100\n",
        "\n",
        "accuracy = compute_model_accuracy(theta_min, X, y)\n",
        "\n",
        "print(f'Model accuracy: {accuracy}%')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model accuracy: 81.46%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}