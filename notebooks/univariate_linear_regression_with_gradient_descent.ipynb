{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "univariate-linear-regression-with-gradient-descent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harperd/machine-learning/blob/master/notebooks/univariate_linear_regression_with_gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY5PT6bd4V65",
        "colab_type": "text"
      },
      "source": [
        "# Univariate Linear Regression and Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zek8oIAP5YQH",
        "colab_type": "text"
      },
      "source": [
        "# Hypothesis Function\n",
        "\n",
        "In machine learning, a *hypothesis* function is used to predict outcomes or $y$ values. Below is the univariate linear regression hypothesis function where theta ($\\theta$) can represent any two numbers and $h_\\theta(x)$ or $y$ is our *prediction*. We just have to figure out what those two numbers are that allow the function to best intersect our data or features. It's a simple linear equation but finding the best theta values is where the challenge lies.\n",
        "\n",
        "> $h_{\\theta }( x) =\\theta _{0} \\ +\\ \\theta _{1} x$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFuhgcny3XzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hypothesis(theta, X):\n",
        "    # Here, X is a Pandas DataFrame containing all of our data.\n",
        "    # X.shape returns tuple of the shape's dimentions in rows x columns.\n",
        "    # Therefore, X.shape[0] will get us the number of rows, or features.\n",
        "    num_features = X.shape[0]\n",
        "    \n",
        "    # numpy.ones returna a new array of given shape and type, filled with ones.\n",
        "    # Here, we want a new array of size num_features x 1 (rows x columns).\n",
        "    h = np.ones(num_features, 1))\n",
        "    \n",
        "    # Loop through each feature (x).\n",
        "    # X.shape() returns tuple of shape (rows, columns) of dataframe/series.\n",
        "    # Here, just want the number of rows.\n",
        "    for i in range(0, num_features):\n",
        "        x = np.concatenate((np.ones(1), np.array([X[i]])), axis = 0)\n",
        "        h[i] = float(np.matmul(theta, x))\n",
        "    \n",
        "    h = h.reshape(X.shape[0])\n",
        "    \n",
        "    return h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-PzzJB86cP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SGD(theta, alpha, num_iters, h, X, y):\n",
        "    for i in range(0,num_iters):\n",
        "        theta[0] = theta[0] - (alpha) * (h - y)\n",
        "        theta[1] = theta[1] - (alpha) * ((h - y) * X)\n",
        "        h = theta[1]*X + theta[0] \n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDI2_-kJ6eh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd_linear_regression(X, y, alpha, num_iters):\n",
        "    # initializing the parameter vector...\n",
        "    theta = np.zeros(2)\n",
        "    # hypothesis calculation....\n",
        "    h = hypothesis(theta, X)    \n",
        "    # returning the optimized parameters by Gradient Descent...\n",
        "    for i in range(0, X.shape[0]):\n",
        "        theta = SGD(theta,alpha,num_iters,h[i],X[i],y[i])\n",
        "    theta = theta.reshape(1, 2)\n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tbEg1AB6hA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.loadtxt('data1.txt', delimiter=',')\n",
        "X_train = data[:,0] #the feature_set\n",
        "y_train = data[:,1] #the labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CukpwQ9N6jQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X_train, y_train)\n",
        "plt.xlabel('Population of City in 10,000s')\n",
        "plt.ylabel('Profit in $10,000s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kg50QML6lWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calling the principal function with learning_rate = 0.0001 and \n",
        "# num_iters = 100000\n",
        "theta = sgd_linear_regression(X_train, y_train, 0.0001, 100000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}