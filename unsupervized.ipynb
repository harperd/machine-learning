{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unsupervized.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harperd/machine-learning/blob/master/unsupervized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "h9A9WTyzIQO9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Machine Learning"
      ]
    },
    {
      "metadata": {
        "id": "_cVrEgkCIQO-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "*Unsupervised Learning* is the assignment of a set of *labeled feature data* (*observations*) into subsets called *clusters* so that observations in the same cluster are similar in some sense. Clustering of data is an optimization problem where a mathematical service or algorithm is used to identify how to best separated the labeled examples. *K-Means* clustering the the most common method which is used in this notebook."
      ]
    },
    {
      "metadata": {
        "id": "TyAr6iUkIQO_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Python Packages\n",
        "\n",
        "The first thing we want to do is import the basic Python packages such as *NumP*, *Matplotlib* and *Pandas*."
      ]
    },
    {
      "metadata": {
        "id": "g5Jw2vBLIQPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# NumPy adds support for large, multi-dimensional arrays and matrices, along with a large collection \n",
        "# of high-level mathematical functions to operate on these arrays.\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib is a plotting library for the Python programming language and its numerical mathematics \n",
        "# extension NumPy. It provides an object-oriented API for embedding plots into applications using \n",
        "# general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pandas is a software library for data manipulation and analysis. In particular, it offers data \n",
        "# structures and operations for manipulating numerical tables and time series.\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JejhMd9DIQPD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Our Data\n",
        "\n",
        "*Pandas* is the primary tool data scientists use for exploring and manipulating data. Most people abbreviate Pandas in their code as *pd*. Here, we import the data into a Pandas *DataFrame* which is the most important part of the Pandas library and will be used to process our data going forward. A *DataFrame* holds the type of data you might think of as a table. This is similar to a sheet in Excel, or a table in a SQL database.\n",
        "\n",
        "Here are the different parts of a data set:\n",
        "\n",
        "- **Labels** - Future outcomes. This is the value we are trying to predict.\n",
        "- **Features (Dimensions)** - All other attributes used to predict Labels. Any attribute chosen to be used as a data point in the training data set. eg. Age, occupation, sex, zip code, etc.\n",
        "- **Feature Vector** - The distinct values of a single feature.\n",
        "- **Example** - A single Feature value or point on the graph or Feature Vector.\n",
        "\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/dataparts.gif?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "5cAZQ8u_IQPD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's configure the data file that will used in this interactive tutorial. For now, this tutorial expects a CSV file but can easily be up dated to accomodate other data sources and file formats."
      ]
    },
    {
      "metadata": {
        "id": "Q3oLAw-bIQPE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_file    = 'https://github.com/harperd/machine-learning/blob/master/data/cardiacData.csv'\n",
        "features     = [ 'HeartRate', 'Att', 'STE', 'Age', 'Outcome' ]\n",
        "data_header  = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5fvEhyg4yLFl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since this is a Cloab notebook we will need to mount our drive first."
      ]
    },
    {
      "metadata": {
        "id": "r7cefboxIQPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First, let's read the file and create our *DataFrame*."
      ]
    },
    {
      "metadata": {
        "id": "olM6W9BHIQPH",
        "colab_type": "code",
        "outputId": "e85a2246-d3bb-4dab-93f0-e87123bd59f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "df_raw = pd.read_csv(data_file, header = data_header, names = features, low_memory = False)\n",
        "\n",
        "%time print(f\"{len(df_raw.index)} rows read.\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2563 rows read.\n",
            "CPU times: user 113 µs, sys: 25 µs, total: 138 µs\n",
            "Wall time: 145 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yU5mCnzTIQPL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can take a quick look at some of the data in our DataFrame to see what it looks like and make sure it loaded okay."
      ]
    },
    {
      "metadata": {
        "id": "CwKibvpmIQPM",
        "colab_type": "code",
        "outputId": "9fb8c483-27c2-4d4d-fc17-20247319d911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "df_raw.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HeartRate</th>\n",
              "      <th>Att</th>\n",
              "      <th>STE</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>106</td>\n",
              "      <td>0</td>\n",
              "      <td>77</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   HeartRate  Att  STE  Age  Outcome\n",
              "0         84    0   55    0        0\n",
              "1         76    0   80    0        0\n",
              "2         50    0   80    0        0\n",
              "3         60    0   44    0        0\n",
              "4        106    0   77    1        0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "eMFVTR_nIQPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Understanding Our Data\n",
        "\n",
        "In any sort of data science work, it's important to look at your data, to make sure you understand the format, how it's stored, what type of values it holds, etc. Even if you've read descriptions about your data, the actual data may not be what you expect."
      ]
    },
    {
      "metadata": {
        "id": "ojUftHkiIQPQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## File Metadata\n",
        "\n",
        "Looking at the data types int the file will help us understand the file format. If we notice a column or *Feature*, that we want to include in our data set that is non-numeric we will want to convert that to a numeric data type when we begin *Feature Engineering* (pre-processing our data)."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "Z0vfYcRTIQPR",
        "colab_type": "code",
        "outputId": "59dc4da6-451e-48d7-979c-3bbccacc813c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "df_raw.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HeartRate    int64\n",
              "Att          int64\n",
              "STE          int64\n",
              "Age          int64\n",
              "Outcome      int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "E00MBEueIQPU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Sample"
      ]
    },
    {
      "metadata": {
        "id": "It8fg-BvIQPV",
        "colab_type": "code",
        "outputId": "4fb9df86-6238-44f1-a48b-5db424b503e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
        "    display(df_raw.tail())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>HeartRate</th>\n",
              "      <th>Att</th>\n",
              "      <th>STE</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "      <td>87</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "      <td>79</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>66</td>\n",
              "      <td>0</td>\n",
              "      <td>89</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>71</td>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     HeartRate  Att  STE  Age  Outcome\n",
              "245         97    1   87    0        1\n",
              "246         78    0   79    0        0\n",
              "247         66    0   89    0        0\n",
              "248         71    0   68    0        0\n",
              "249         73    0   49    1        0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "KtcudmH0IQPY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Describing Our Data\n",
        "\n",
        "The first number, the count, shows how many rows have non-missing values.\n",
        "\n",
        "Missing values arise for many reasons. For example, the size of the 2nd bedroom wouldn't be collected when surveying a 1 bedroom house. We'll come back to the topic of missing data.\n",
        "\n",
        "The second value is the mean, which is the average. Under that, std is the standard deviation, which measures how numerically spread out the values are.\n",
        "\n",
        "To interpret the min, 25%, 50%, 75% and max values, imagine sorting each column from lowest to highest value. The first (smallest) value is the min. If you go a quarter way through the list, you'll find a number that is bigger than 25% of the values and smaller than 75% of the values. That is the 25% value (pronounced \"25th percentile\"). The 50th and 75th percentiles are defined analogously, and the max is the largest number."
      ]
    },
    {
      "metadata": {
        "id": "07PwasrRIQPZ",
        "colab_type": "code",
        "outputId": "bd513b62-350e-4570-c202-e134cd6025a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "df_raw.describe(include='all').T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>HeartRate</th>\n",
              "      <td>250.0</td>\n",
              "      <td>70.008</td>\n",
              "      <td>14.472795</td>\n",
              "      <td>26.0</td>\n",
              "      <td>61.00</td>\n",
              "      <td>70.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>110.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Att</th>\n",
              "      <td>250.0</td>\n",
              "      <td>0.104</td>\n",
              "      <td>0.305873</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>STE</th>\n",
              "      <td>250.0</td>\n",
              "      <td>65.548</td>\n",
              "      <td>14.091486</td>\n",
              "      <td>23.0</td>\n",
              "      <td>55.25</td>\n",
              "      <td>67.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>99.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Age</th>\n",
              "      <td>250.0</td>\n",
              "      <td>0.360</td>\n",
              "      <td>0.513278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Outcome</th>\n",
              "      <td>250.0</td>\n",
              "      <td>0.332</td>\n",
              "      <td>0.471876</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           count    mean        std   min    25%   50%   75%    max\n",
              "HeartRate  250.0  70.008  14.472795  26.0  61.00  70.0  78.0  110.0\n",
              "Att        250.0   0.104   0.305873   0.0   0.00   0.0   0.0    1.0\n",
              "STE        250.0  65.548  14.091486  23.0  55.25  67.0  76.0   99.0\n",
              "Age        250.0   0.360   0.513278   0.0   0.00   0.0   1.0    2.0\n",
              "Outcome    250.0   0.332   0.471876   0.0   0.00   0.0   1.0    1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "QbjJQareIQPb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Visualizing Our Data"
      ]
    },
    {
      "metadata": {
        "id": "khOgFDZ4IQPc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualizing the data is key to helping understand the makeup of the data and it's attributes (Feature Vectors). This helps to find ways to tune the model as well as outliers or data that may need standardized."
      ]
    },
    {
      "metadata": {
        "id": "zTYYRM6JIQPd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Histograms"
      ]
    },
    {
      "metadata": {
        "id": "r5FSkJbSIQPe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A fast way to get an idea of the shape of the data, which is to say, the distribution (or density) of each *feature* is to look at histograms.\n",
        "\n",
        "Histograms represent the distribution of a continuous variable (*feature* in this case) over a given interval or period of time. Histograms plot the data by dividing it into intervals called *bins*. It is used to inspect the underlying frequency distribution or shape of the data such as Gaussian (eg. Normal distribution), outliers, skewness, etc.\n",
        "\n",
        "In a histogram, it is the area of the bar that indicates the frequency of occurrences for each bin. This means that the height of the bar does not necessarily indicate how many occurrences of scores there were within each individual bin. It is the product of height multiplied by the width of the bin that indicates the frequency of occurrences within that bin. \n",
        "\n",
        "\n",
        "[Types of distributions](https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/)\n",
        "\n",
        "\n",
        "**Choosing a Bin Number**\n",
        "\n",
        "Setting bins to a higher or lower number will yeild denser or sparser results. The choice of bin width significantly affects the resulting plot. Smaller binwidths can make the plot cluttered, but larger binwidths may obscure nuances in the data.\n",
        "\n",
        "*Matplotlib* will automatically choose a reasonable binwidth for you, but you can specify the binwidth yourself after trying out several values. There is no true right or wrong answer, you just have to try different bin values and see which works best for your particular data.\n",
        "\n",
        "> **TIP**: If you have a small amount of data, use wider bins to eliminate noise. If you have a lot of data, use narrower bins because the histogram will not be that noisy.\n",
        "\n",
        "[](images/magnifying-glass.png) **So, what are we looking for?** Normal *Guassian* distributions are preferred to work with since data tends to cluster at a given point. This is necessary when using a clustering algorithm, such as *K-Means*. If we don't see good clusting of feature data, then we may need to get more data or the data may not be able to used. The image below depicts a normal distribution of data.\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/normal-distribution.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "tkgQqA-PIQPe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Using the Data Frame we will produce histograms of each attribute\n",
        "# with an optional specified number of bins. Matplotlib will automatically\n",
        "# choose a reasonable binwidth for you if set to None.\n",
        "axarr = df_raw.hist(histtype = 'bar', bins = 10)\n",
        "\n",
        "for ax in axarr.flatten():\n",
        "    feature = ax.get_title()\n",
        "    ax.set_title(f'Histogram of {feature}')\n",
        "    ax.set_xlabel(feature)\n",
        "    ax.set_ylabel('Frequency')\n",
        "    \n",
        "plt.subplots_adjust(left = 3, right = 5, top = 12, bottom = 9, wspace = None, hspace = None)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G6SNS5TRIQPi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Density Plots\n",
        "\n",
        "There is also another, often clearer, way to grasp the distribution: density plots or, more formally, *Kernel Density Plots*. They can be considered a smoothed version of the histogram. Their main advantage over the latter is that they do not depend on the size of the bins.\n",
        "\n",
        "**So, what are we looking for?** Here again, we want to see if there are any normal distributions of the data."
      ]
    },
    {
      "metadata": {
        "id": "Yecrt-mcIQPj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_raw[features].plot(\n",
        "    kind = 'density', \n",
        "    # A tuple (rows, columns) for the layout of subplots.\n",
        "    layout = (len(features), 2), \n",
        "    # A tuple (width, height) in inches.\n",
        "    figsize = (20, 30), \n",
        "    # Make separate subplots for each column.\n",
        "    subplots = True, \n",
        "    # In case subplots=True, share x axis and set some x axis labels to invisible; \n",
        "    # defaults to True if ax is None otherwise False if an ax is passed in; Be aware, \n",
        "    # that passing in both an ax and sharex=True will alter all x axis labels for \n",
        "    # all axis in a figure!\n",
        "    sharex = False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hdjHJ5LPIQPm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Box-And-Whisker Plot\n",
        "\n",
        "A *Box Plot* or *Box and Whisker* plot is good for understanding various aspects of a *Feature* in a single graph. This includes the median, 1st and 3rd quartiles (25% and 75% of the data so what is inside the box is 50% of the data or *Examples*), the maximum value and the minimum value.\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/boxplot.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "3WFFokwVIQPn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_raw.plot(\n",
        "    kind = 'box',\n",
        "    # A tuple (rows, columns) for the layout of subplots.\n",
        "    layout = (len(features), 5), \n",
        "    # A tuple (width, height) in inches.\n",
        "    figsize = (20, 30), \n",
        "    # Make separate subplots for each column.\n",
        "    subplots = True, \n",
        "    # In case subplots=True, share x axis and set some x axis labels to invisible; \n",
        "    # defaults to True if ax is None otherwise False if an ax is passed in; Be aware, \n",
        "    # that passing in both an ax and sharex=True will alter all x axis labels for \n",
        "    # all axis in a figure!\n",
        "    sharex = False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hhgJmnvsIQPs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Scatter Matrix"
      ]
    },
    {
      "metadata": {
        "id": "EsLmo3hcIQPt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "scatter_matrix(df_raw)\n",
        "plt.subplots_adjust(left = 3, right = 5, top = 12, bottom = 10, wspace = None, hspace = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n3EyoHxpIQPy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "metadata": {
        "id": "8W4RwAPfIQP0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Clustering\n",
        "\n",
        "Clustering, in Machine Learning, is the assignment of a set of *observations* into subsets (called *clusters*) so that observations in the same cluster are similar in some sense. Clustering is a method of *unsupervised learning*, and a common technique for statistical data analysis used in many fields. Clustering is an optimization problem. We need to identify the best cluster of *Features*. When given *labeled data* we need identify the mathematical service that best separates the labeled examples. However, this is subject to constraints to avoid overfitting. \n",
        "\n",
        "For labeled data we want to find the boundaries in Feature space that separate different classes of labeled Examples. New data assigned to class based on it’s portion of the Feature space carved out by classifier surface in which it lies.\n",
        "\n",
        "- Look for a simple surface (e.g. best line or plane) that separates the classes or clusters.\n",
        "- Look for more complex surfaces, subject to constraints, that separate classes.\n",
        "- Use K nearest neighbor (voting scheme) which use majority vote to select labels.\n",
        "\n",
        "*K-Means* is the most common Unsupervised clustering algorithm.\n",
        "\n",
        "- Useful when the number of necessary clusters (k) is known.\n",
        "- Faster than Hierarchical such as Random Forests."
      ]
    },
    {
      "metadata": {
        "id": "--YqH14MIQP1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clustering Constraints\n",
        "\n",
        "Learned models (clusters) will depend on certain constraints:\n",
        "\n",
        "- The distance metric between examples (Minkowski + Linkage)\n",
        "- Which Features to include (Feature Vectors)\n",
        "- Constraints on the complexity of the model to avoid overfitting (each Example is its own cluster):\n",
        "- The specified number of clusters (k)\n",
        "- Complexity of the separating surface"
      ]
    },
    {
      "metadata": {
        "id": "2jgPyeHiIQP1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Basic Logic\n",
        "\n",
        "1. Randomly choose k examples as initial centroids\n",
        "2. For each centroid, create k clusters by assigning each example to the closest centroid.\n",
        "3. Compute k new centroids by averaging examples in each cluster.\n",
        "4. Continue until centroids don’t change."
      ]
    },
    {
      "metadata": {
        "id": "YC09GLkDIQP2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# K-Means Clustering\n",
        "\n",
        "*K-Means* is an unsupervised machine learning algorithm (data is labeled) that groups a dataset into a user-specified number of clusters. This algorithm is somewhat naive as it clusters the data into a user specified k clusters, even if k is not the right number of clusters to use. Therefore, it is sometimes a challenge to find the optimal value for k. An optimial k is one where increasing k doesn't give much better model of the data."
      ]
    },
    {
      "metadata": {
        "id": "9-U1o7HqIQP3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Function for K-Means clustering used in the following sections.\n",
        "def cluster_kmeans(df, k):\n",
        "    model = KMeans(\n",
        "        #\n",
        "        # KMeans Hyperparameters\n",
        "        #\n",
        "        \n",
        "        # The number of clusters to form as well as the number of centroids to generate.\n",
        "        n_clusters = k, \n",
        "        # Method for initialization, defaults to ‘k-means++’:\n",
        "        # ‘k-means++’: selects initial cluster centers for k-mean clustering in a smart way to \n",
        "        #              speed up convergence.\n",
        "        # ‘random’: choose k observations (rows) at random from data for the initial centroids.\n",
        "        init = 'k-means++',\n",
        "        # Maximum number of iterations of the k-means algorithm for a single run.\n",
        "        max_iter = 300,\n",
        "        # Number of time the k-means algorithm will be run with different centroid seeds. \n",
        "        # The final results will be the best output of n_init consecutive runs in terms of inertia.\n",
        "        n_init = 10,\n",
        "        # Determines random number generation for centroid initialization. Use an int to \n",
        "        # make the randomness deterministic. \n",
        "        random_state = 0)\n",
        "    # Compute k-means clustering.\n",
        "    return model.fit(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JjWeSUBfIQP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Measuring Distance Between Clusters\n",
        "\n",
        "Before we get too far into clustering we should cover how distances between features vectors and clusters is determined. Distance between clusters is computed by identifying which features to measure distance between (Linkage Metric) and measuring the actual distance between the identified features using the Minkowski method.\n",
        "\n",
        "- **Single-Linkage** considers the distance between clusters using the shortest distance from any member (example) of one cluster to any member of the other cluster.\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/single-linkage.jpg?raw=1)\n",
        "\n",
        "- **Complete-Linkage** considers the distance between clusters using the greatest distance from any member (example) of one cluster to any member of the other cluster.\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/complete-linkage.jpg?raw=1)\n",
        "\n",
        "- **Average-Linkage** considers the distance between clusters using the average distance from any member (example) of one cluster to any member of the other cluster.\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/average-linkage.jpg?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "JmMcgvHtIQP5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Minkowski Metric\n",
        "\n",
        "This is the basis for finding distances between Features (or Examples) for *Euclidean*, which is the most common, and *Manhattan* distances:\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/minkowski.jpg?raw=1)\n",
        "Two different ways to use the equation. For *Euclidean Distance* (straight line between two points) use p = 2, For *Manhattan Distance* (along axises only) use p = 1.\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/manhattan-euclidean.jpg?raw=1)\n",
        "\n",
        "Euclidean is mostly used. Manhattan is sometimes used when different Features (or Dimensions) are not comparable or higher dimensional data. For example, age vs. other Features that are binary in nature, such as “wears glasses”.\n",
        "\n",
        "**Other distance metrics:** Chebyshev, Cosine, Canberra"
      ]
    },
    {
      "metadata": {
        "id": "BZXHecfzIQP6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.spatial.distance as dist\n",
        "\n",
        "model = cluster_kmeans(df_raw, k = 3)\n",
        "\n",
        "# Create some clusters with random data\n",
        "cluster_A = model.cluster_centers_[0]\n",
        "cluster_B = model.cluster_centers_[1]\n",
        "dimensions = len(cluster_A)\n",
        "\n",
        "print(f'Distance measurements with {dimensions}-dimensional vectors')\n",
        "print()\n",
        "print('Euclidean distance is', dist.euclidean(cluster_A, cluster_B))\n",
        "print('Manhattan distance is', dist.cityblock(cluster_A, cluster_B))\n",
        "print('Chebyshev distance is', dist.chebyshev(cluster_A, cluster_B))\n",
        "print('Canberra distance is', dist.canberra(cluster_A, cluster_B))\n",
        "print('Cosine distance is', dist.cosine(cluster_A, cluster_B))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JWODXRwLIQP8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Choosing K\n",
        "\n",
        "There are different ways to choose a number of clusters (k) such as have a priori knowledge about the application domain. But that is more than likely not the case in the real world. You can use hierarchical clustering on a subset of data where each leaf node in a tree is a cluster but this is slow on larger datasets and not an optimal way of choosing a value for k. Other more common ways are outlined in detail in the following sections.\n",
        "\n",
        "Things to consider when choosing K:\n",
        "\n",
        "- Results can depend on location of initial centroids.\n",
        "- Choosing the wrong k value can lead to inaccurate results.\n",
        "- Hyperparameters may need to be tuned to get a better model fit."
      ]
    },
    {
      "metadata": {
        "id": "k_z7D2xJIQP9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Method 1: Measuring Model Optimization\n",
        "\n",
        "One way to find an optimal value for k is int trying different values of k and evaluate quality of results (dissimilarity).\n",
        "\n",
        "**Cluster Variability** is the sum of the distance between the cluster mean (centroid) and each example then square the result. A low value denotes a tight cluster. 'c' denotes a single cluster and 'e' denotes a single vector within the cluster.\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/variability.jpg?raw=1)\n",
        "**Model Dissimilarity** is the sum of all the cluster variabilities. High model dissimilarities result in an underfit model so constraints need to be applied as “guardrails” to provide a more fit model such as fewer clusters (k) or some feature values need to be scaled. 'c' denotes a single cluster, 'C' denotes all clusters or the model and 'e' denotes a single vector within the cluster.\n",
        "\n",
        "**NOTE:** Overfitting and underfitting are the two biggest causes for poor performance of machine learning algorithms. Ideally, you want to select a model at the sweet spot between underfitting and overfitting.\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/dissimilarity.jpg?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "cQ8mZCN-IQP-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Cluster Variability\n",
        "\n",
        "To measture the dissimilarity of the model *C* we need to first create a function that will measure the variaiblity of a single cluster. We will use Euclidean Distance as it is more common."
      ]
    },
    {
      "metadata": {
        "id": "e583KHshIQP_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.spatial.distance as dist\n",
        "\n",
        "def variability(c):\n",
        "    v = 0\n",
        "    centroid = c.mean()\n",
        "    for e in c:\n",
        "         v = v + ( dist.euclidean(centroid, e) ** 2 )\n",
        "    return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b6oVNITnIQQB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Model Dissimilarity\n",
        "\n",
        "Now that we have a function to measture the variaiblity of each cluster we can then employ that function to measure the dissimilarity of our model."
      ]
    },
    {
      "metadata": {
        "id": "ozB1xgF6IQQC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def dissimilarity(C):\n",
        "    d = 0\n",
        "    for c in C:\n",
        "        d = d + variability(c)\n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5-O8rkUzIQQG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's try some different k values to see which one seems optimial.\n",
        "\n",
        "**So, what are we looking for?** We want to identify the k value just before the dissimilarity value starts showing our model is starting to underfit. Too low of a value means the model is overfit. A good k value is somewhere in the middle. Using the below as an example, we would choose a k of 3.\n",
        "\n",
        "- Model dissimilarity with k = 1 is 4479.537427199999\n",
        "- Model dissimilarity with k = 2 is 10276.53491116935\n",
        "- Model dissimilarity with k = 3 is 15824.206394492456\n",
        "- Model dissimilarity with k = 4 is 24273.153638284602\n",
        "- Model dissimilarity with k = 5 is 30715.503779356583"
      ]
    },
    {
      "metadata": {
        "id": "XZzawGaJIQQH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_variabilities(C):\n",
        "    # Output the variability of each cluster. Not nessary but provides more detail.\n",
        "    i = 1\n",
        "\n",
        "    for c in C:\n",
        "        print(f\"\\tCluster {i} variability is {variability(c)}\")\n",
        "        i = i + 1\n",
        "\n",
        "def print_dissimilarity(model, show_variability):\n",
        "    C = model.cluster_centers_\n",
        "    k = len(C)\n",
        "    \n",
        "    # Output the model dissimilarity\n",
        "    print(f\"Model dissimilarity with k = {k} is {dissimilarity(C)}\")\n",
        "    \n",
        "    if(show_variability):\n",
        "        print()\n",
        "        print_variabilities(C)\n",
        "        print()\n",
        "        \n",
        "# k values we what to try\n",
        "samples = 6\n",
        "\n",
        "for k in range(1, samples):\n",
        "    model = cluster_kmeans(df_raw, k)\n",
        "    print_dissimilarity(model, show_variability = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cLFDXvZrIQQL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###  Method 2: Elbow Method (Sum of Squared Errors)\n",
        "\n",
        "This method looks at the percentage of variance explained as a function of the number of clusters. Achieving this yeilds and optimal value for k.\n",
        "\n",
        "The idea of the elbow method is to run K-Means clustering on the dataset for a range of values of k (k from 1 to 10 for example). Then, for each value of k, calculate the *Sum of Squared Errors (SSE)* or *Within Cluster Sum of Squares (WCSS)*.\n",
        "\n",
        "The smaller the SSE the better and the SSE tends to decrease toward 0 as we increase k. When SSE is 0 then k is equal to the number of data points (or *Examples*) in the dataset. This is because each data point is its own cluster (*Over Fitting*), and there is no error between it and the center of its cluster.\n",
        "\n",
        "**So, what are we looking for?** The goal is to choose a small value of k that still has a low SSE. The elbow usually represents where we start to have diminishing returns by increasing k. In the example below, a k of 3 is the optimal number.\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/elbow-method.png?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "Z-DBkEP8IQQM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sse = []\n",
        "samples = 9\n",
        "\n",
        "# Try different values for k and record the results.\n",
        "for k in range(1, samples):\n",
        "    model = cluster_kmeans(df_raw, k)\n",
        "    # interia_ is the sum of squared distances of samples \n",
        "    # to their closest cluster center.\n",
        "    sse.append(model.inertia_)\n",
        "    display(model.inertia_)\n",
        "    \n",
        "# Plotting the results onto a line graph, allowing us to observe 'The elbow'\n",
        "plt.plot(range(1, samples), sse)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters K')\n",
        "plt.ylabel('Sum of Squared Errors')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-t2guiOmIQQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Method 3: Average Silhouette Method\n",
        "\n",
        "*Silhouette Analysis* can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n",
        "\n",
        "Silhouette coefficients (as these values are referred to as) are:\n",
        "\n",
        "- **Near +1** indicate that the sample is far away from the neighboring clusters.\n",
        "- **0** indicates that the sample is on or very close to the decision boundary between two neighboring clusters.\n",
        "- **Negative values** indicate that those samples might have been assigned to the wrong cluster."
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "cuijft6dIQQU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "#print(__doc__)\n",
        "\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "X = df_raw.values\n",
        "samples = 10\n",
        "\n",
        "for k in range(2, samples):\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    \n",
        "    # The (k+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (k + 1) * 10])\n",
        "\n",
        "    # Initialize the model with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    model = cluster_kmeans(df_raw, k)\n",
        "    cluster_labels = model.predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters.\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(f'When k = {k} the average silhouette_score is: {silhouette_avg}')\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(k):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / k)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor = color, edgecolor = color, alpha = 0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title('The silhouette plot for the various clusters.')\n",
        "    ax1.set_xlabel('The silhouette coefficient values')\n",
        "    ax1.set_ylabel('Cluster label')\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x = silhouette_avg, color = 'red', linestyle = '--')\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / k)\n",
        "    ax2.scatter(X[:, 0], X[:, 1], marker = '.', s = 30, lw = 0, alpha = 0.7,\n",
        "                c = colors, edgecolor = 'k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = model.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker = 'o',\n",
        "                c = 'white', alpha = 1, s = 200, edgecolor = 'k')\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker = '$%d$' % i, alpha = 1,\n",
        "                    s = 50, edgecolor = 'k')\n",
        "\n",
        "    ax2.set_title('The visualization of the clustered data.')\n",
        "    # ax2.set_xlabel('Feature space for the 1st feature')\n",
        "    # ax2.set_ylabel('Feature space for the 2nd feature')\n",
        "\n",
        "    plt.suptitle(('Silhouette analysis for KMeans clustering on sample data '\n",
        "                  'with k = %d' % k),\n",
        "                 fontsize = 14, fontweight = 'bold')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KSPWZLmKIQQX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Method 4: Gap Statistic Method"
      ]
    },
    {
      "metadata": {
        "id": "4XM-eQuYIQQX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Method 5: G-Means Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "_cztU7j2IQQY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creating The Cluster\n",
        "\n",
        "Now that we have a good value for k we can go ahead and create our model."
      ]
    },
    {
      "metadata": {
        "id": "0dR-I07GIQQZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%time model = cluster_kmeans(df_raw, k = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yqMo9nzMIQQc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Measuring Model Accuracy\n",
        "\n",
        "Once you have a model you can run your test data and product a *Confusion Matrix* or *Error Matrix* which will be used to determine training errors.\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/confusion-matrix.jpg?raw=1)\n",
        "\n",
        "The accuracy is then measured by the below. This should be done for not only the training data set but to the test data set.\n",
        "![](https://github.com/harperd/machine-learning/blob/master/images/accuracy.jpg?raw=1)"
      ]
    },
    {
      "metadata": {
        "id": "vTkEm-T9IQQd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}